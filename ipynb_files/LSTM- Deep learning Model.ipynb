{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "518c90df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\akank\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\akank\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "#for data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "\n",
    "#for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "#for nlp\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from gensim import corpora\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import SpatialDropout1D\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import (precision_score, recall_score,\n",
    "                             f1_score, accuracy_score,mean_squared_error,mean_absolute_error)\n",
    "np.random.seed(0)\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import callbacks\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cb61200",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load train data\n",
    "cols = ['id', 'text', 'label', 'intensity']\n",
    "\n",
    "anger_train = pd.read_csv(\"C:/projects/Train/anger-ratings-0to1.train.txt\", header=None, sep='\\t', names=cols, index_col=0)\n",
    "fear_train = pd.read_csv(\"C:/projects/Train/fear-ratings-0to1.train.txt\", header=None, sep='\\t', names=cols, index_col=0)\n",
    "sad_train = pd.read_csv(\"C:/projects/Train/joy-ratings-0to1.train.txt\", header=None, sep='\\t', names=cols, index_col=0)\n",
    "joy_train = pd.read_csv(\"C:/projects/Train/sadness-ratings-0to1.train.txt\", header=None, sep='\\t', names=cols, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2f3ce05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dev data\n",
    "cols = ['id', 'text', 'label', 'intensity']\n",
    "\n",
    "anger_test = pd.read_csv(\"C:/projects/Validation/anger-ratings-0to1.dev.gold.txt\", header=None, sep='\\t', names=cols, index_col=0)\n",
    "fear_test = pd.read_csv(\"C:/projects/Validation/fear-ratings-0to1.dev.gold.txt\", header=None, sep='\\t', names=cols, index_col=0)\n",
    "sad_test = pd.read_csv(\"C:/projects/Validation/joy-ratings-0to1.dev.gold.txt\", header=None, sep='\\t', names=cols, index_col=0)\n",
    "joy_test = pd.read_csv(\"C:/projects/Validation/sadness-ratings-0to1.dev.gold.txt\", header=None, sep='\\t', names=cols, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b59d9720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>intensity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>How the fu*k! Who the heck! moved my fridge!.....</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10001</th>\n",
       "      <td>So my Indian Uber driver just called someone t...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label  intensity\n",
       "id                                                                        \n",
       "10000  How the fu*k! Who the heck! moved my fridge!.....  anger      0.938\n",
       "10001  So my Indian Uber driver just called someone t...  anger      0.896"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anger_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "619a13d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([anger_train, fear_train, sad_train, joy_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7784b137",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.concat([anger_test, fear_test, sad_test, joy_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5651fed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class TextCleaner:\n",
    "\n",
    "    def __init__(self, text: str):\n",
    "        self.text = text\n",
    "\n",
    "    def fully_clean(self):\n",
    "        return self \\\n",
    "            .remove_urls() \\\n",
    "            .remove_mentions() \\\n",
    "            .remove_hashtags() \\\n",
    "            .remove_reserved_words() \\\n",
    "            .remove_punctuation() \\\n",
    "            .remove_single_letter_words() \\\n",
    "            .remove_blank_spaces() \\\n",
    "            .remove_stopwords() \\\n",
    "            .remove_numbers() \\\n",
    "            .remove_non_ascii() \\\n",
    "            .lowercase()\n",
    "\n",
    "    def remove_urls(self):\n",
    "        url_pattern = re.compile(\n",
    "            r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))'\n",
    "            r'[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]\\.[^\\s]{2,})')\n",
    "        self.text = re.sub(pattern=url_pattern, repl='', string=self.text)\n",
    "        return self\n",
    "\n",
    "    def remove_punctuation(self):\n",
    "        self.text = self.text.translate(str.maketrans('', '', string.punctuation))\n",
    "        return self\n",
    "\n",
    "    def remove_mentions(self):\n",
    "        mentions_pattern = re.compile(r'@\\w*')\n",
    "        self.text = re.sub(pattern=mentions_pattern, repl='', string=self.text)\n",
    "        return self\n",
    "\n",
    "    def remove_hashtags(self):\n",
    "        hashtags_pattern = re.compile(r'#\\w*')\n",
    "        self.text = re.sub(pattern=hashtags_pattern, repl='', string=self.text)\n",
    "        return self\n",
    "\n",
    "    def remove_reserved_words(self):\n",
    "        reserved_words_pattern = re.compile(r'(RT|rt|FAV|fav|VIA|via)')\n",
    "        self.text = re.sub(pattern=reserved_words_pattern, repl='', string=self.text)\n",
    "        return self\n",
    "\n",
    "    def remove_single_letter_words(self):\n",
    "        single_letter_words_pattern = re.compile(r'(?<![\\w\\-])\\w(?![\\w\\-])')\n",
    "        self.text = re.sub(pattern=single_letter_words_pattern, repl='', string=self.text)\n",
    "        return self\n",
    "\n",
    "    def remove_blank_spaces(self):\n",
    "        blank_spaces_pattern = re.compile(r'\\s{2,}|\\t')\n",
    "        self.text = re.sub(pattern=blank_spaces_pattern, repl=' ', string=self.text)\n",
    "        return self\n",
    "\n",
    "    def remove_stopwords(self, extra_stopwords=None):\n",
    "        if extra_stopwords is None:\n",
    "            extra_stopwords = []\n",
    "        tokenizer = TweetTokenizer()\n",
    "        words = tokenizer.tokenize(self.text)\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "\n",
    "        new_sentence = [w for w in words if w.lower() not in stop_words and w.lower() not in extra_stopwords]\n",
    "        self.text = ' '.join(new_sentence)\n",
    "        return self\n",
    "\n",
    "    def remove_numbers(self, preserve_years=False):\n",
    "        text_list = self.text.split(' ')\n",
    "        for text in text_list[:]:\n",
    "            if text.isnumeric():\n",
    "                if preserve_years:\n",
    "                    if not is_year(text):\n",
    "                        text_list.remove(text)\n",
    "                else:\n",
    "                    text_list.remove(text)\n",
    "\n",
    "        self.text = ' '.join(text_list)\n",
    "        return self\n",
    "    \n",
    "    def is_year(self, text):\n",
    "        try:\n",
    "            year = int(text)\n",
    "            return 1000 <= year <= 9999\n",
    "        except ValueError:\n",
    "            return False\n",
    "    \n",
    "    def remove_non_ascii(self):\n",
    "        self.text = ''.join(char for char in self.text if ord(char) < 128)\n",
    "        return self\n",
    "\n",
    "    def lowercase(self):\n",
    "        self.text = self.text.lower()\n",
    "        return self\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30e44d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>intensity</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>How the fu*k! Who the heck! moved my fridge!.....</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.938</td>\n",
       "      <td>fuk heck moved fridge knock landlord door</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10001</th>\n",
       "      <td>So my Indian Uber driver just called someone t...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.896</td>\n",
       "      <td>indian uber driver called someone word wasnt m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10002</th>\n",
       "      <td>@DPD_UK I asked for my parcel to be delivered ...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.896</td>\n",
       "      <td>asked parcel delivered pick store address</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10003</th>\n",
       "      <td>so ef whichever butt wipe pulled the fire alar...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.896</td>\n",
       "      <td>ef whichever butt wipe pulled fire alarm davis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10004</th>\n",
       "      <td>Don't join @BTCare they put the phone down on ...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.896</td>\n",
       "      <td>dont join put phone talk rude taking money acc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label  intensity  \\\n",
       "id                                                                           \n",
       "10000  How the fu*k! Who the heck! moved my fridge!.....  anger      0.938   \n",
       "10001  So my Indian Uber driver just called someone t...  anger      0.896   \n",
       "10002  @DPD_UK I asked for my parcel to be delivered ...  anger      0.896   \n",
       "10003  so ef whichever butt wipe pulled the fire alar...  anger      0.896   \n",
       "10004  Don't join @BTCare they put the phone down on ...  anger      0.896   \n",
       "\n",
       "                                              clean_text  \n",
       "id                                                        \n",
       "10000          fuk heck moved fridge knock landlord door  \n",
       "10001  indian uber driver called someone word wasnt m...  \n",
       "10002          asked parcel delivered pick store address  \n",
       "10003  ef whichever butt wipe pulled fire alarm davis...  \n",
       "10004  dont join put phone talk rude taking money acc...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import TweetTokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# Rest of your code\n",
    "text = train_df['text']\n",
    "clean_text = []\n",
    "\n",
    "for text_entry in text:\n",
    "    c = TextCleaner(text_entry)\n",
    "    c.fully_clean()\n",
    "    cleaned_text = c.text\n",
    "    clean_text.append(cleaned_text)\n",
    "\n",
    "train_df['clean_text'] = clean_text \n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ead12be6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>intensity</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10857</th>\n",
       "      <td>@ZubairSabirPTI  pls dont insult the word 'Molna'</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.479</td>\n",
       "      <td>pls dont insult word molna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10858</th>\n",
       "      <td>@ArcticFantasy I would have almost took offens...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.458</td>\n",
       "      <td>would almost took offense actually snapped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10859</th>\n",
       "      <td>@IllinoisLoyalty that Rutgers game was an abom...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.562</td>\n",
       "      <td>rutgers game abomination affront god man must ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10860</th>\n",
       "      <td>@CozanGaming that's what lisa asked before she...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.500</td>\n",
       "      <td>thats lisa asked staed raging call heh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10861</th>\n",
       "      <td>Sometimes I get mad over something so minuscul...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.708</td>\n",
       "      <td>sometimes get mad something minuscule try ruin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label  intensity  \\\n",
       "id                                                                           \n",
       "10857  @ZubairSabirPTI  pls dont insult the word 'Molna'  anger      0.479   \n",
       "10858  @ArcticFantasy I would have almost took offens...  anger      0.458   \n",
       "10859  @IllinoisLoyalty that Rutgers game was an abom...  anger      0.562   \n",
       "10860  @CozanGaming that's what lisa asked before she...  anger      0.500   \n",
       "10861  Sometimes I get mad over something so minuscul...  anger      0.708   \n",
       "\n",
       "                                              clean_text  \n",
       "id                                                        \n",
       "10857                         pls dont insult word molna  \n",
       "10858         would almost took offense actually snapped  \n",
       "10859  rutgers game abomination affront god man must ...  \n",
       "10860             thats lisa asked staed raging call heh  \n",
       "10861  sometimes get mad something minuscule try ruin...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# Rest of your code\n",
    "text = test_df['text']\n",
    "clean_text = []\n",
    "\n",
    "for text_entry in text:\n",
    "    c = TextCleaner(text_entry)\n",
    "    c.fully_clean()\n",
    "    cleaned_text = c.text\n",
    "    clean_text.append(cleaned_text)\n",
    "\n",
    "test_df['clean_text'] = clean_text \n",
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ea80373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "226/226 [==============================] - ETA: 0s - loss: 0.0537 - mean_absolute_error: 0.1809\n",
      "Epoch 1: val_loss improved from inf to 0.03482, saving model to bestmodel_weights.h5\n",
      "226/226 [==============================] - 44s 131ms/step - loss: 0.0537 - mean_absolute_error: 0.1809 - val_loss: 0.0348 - val_mean_absolute_error: 0.1502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akank\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "226/226 [==============================] - ETA: 0s - loss: 0.0359 - mean_absolute_error: 0.1531\n",
      "Epoch 2: val_loss improved from 0.03482 to 0.03186, saving model to bestmodel_weights.h5\n",
      "226/226 [==============================] - 28s 125ms/step - loss: 0.0359 - mean_absolute_error: 0.1531 - val_loss: 0.0319 - val_mean_absolute_error: 0.1444\n",
      "Epoch 3/100\n",
      "226/226 [==============================] - ETA: 0s - loss: 0.0331 - mean_absolute_error: 0.1472\n",
      "Epoch 3: val_loss did not improve from 0.03186\n",
      "226/226 [==============================] - 29s 126ms/step - loss: 0.0331 - mean_absolute_error: 0.1472 - val_loss: 0.0322 - val_mean_absolute_error: 0.1448\n",
      "Epoch 4/100\n",
      "226/226 [==============================] - ETA: 0s - loss: 0.0327 - mean_absolute_error: 0.1455\n",
      "Epoch 4: val_loss did not improve from 0.03186\n",
      "226/226 [==============================] - 29s 128ms/step - loss: 0.0327 - mean_absolute_error: 0.1455 - val_loss: 0.0346 - val_mean_absolute_error: 0.1491\n",
      "Epoch 5/100\n",
      "226/226 [==============================] - ETA: 0s - loss: 0.0332 - mean_absolute_error: 0.1474\n",
      "Epoch 5: val_loss did not improve from 0.03186\n",
      "226/226 [==============================] - 29s 127ms/step - loss: 0.0332 - mean_absolute_error: 0.1474 - val_loss: 0.0324 - val_mean_absolute_error: 0.1483\n",
      "Epoch 6/100\n",
      "226/226 [==============================] - ETA: 0s - loss: 0.0335 - mean_absolute_error: 0.1474\n",
      "Epoch 6: val_loss did not improve from 0.03186\n",
      "226/226 [==============================] - 218s 967ms/step - loss: 0.0335 - mean_absolute_error: 0.1474 - val_loss: 0.0332 - val_mean_absolute_error: 0.1485\n",
      "Epoch 7/100\n",
      "226/226 [==============================] - ETA: 0s - loss: 0.0339 - mean_absolute_error: 0.1483\n",
      "Epoch 7: val_loss did not improve from 0.03186\n",
      "226/226 [==============================] - 25s 112ms/step - loss: 0.0339 - mean_absolute_error: 0.1483 - val_loss: 0.0331 - val_mean_absolute_error: 0.1474\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x25d8ba416a0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import SpatialDropout1D, Dropout, LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# # Assuming your 'intensity' values are continuous, not categorical\n",
    "# intensity_values = train_df['intensity'].values\n",
    "# test_intensity_values = test_df['intensity'].values\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer(num_words=500, split=' ') \n",
    "tokenizer.fit_on_texts(train_df['clean_text'].values)  # Corrected line\n",
    "\n",
    "X = tokenizer.texts_to_sequences(train_df['clean_text'].values)\n",
    "X = pad_sequences(X)\n",
    "\n",
    "\n",
    "# Convert labels to regression values\n",
    "intensity_values = train_df['intensity'].values\n",
    "\n",
    "# Preprocess test data\n",
    "test_sequences = tokenizer.texts_to_sequences(test_df['clean_text'].values)\n",
    "test_data_padded = pad_sequences(test_sequences, maxlen=X.shape[1])\n",
    "\n",
    "# Convert test labels to regression values\n",
    "intensity_values_test = test_df['intensity'].values\n",
    "\n",
    "# Model Building- best combo tried\n",
    "model = Sequential()\n",
    "model.add(Embedding(3666, 120, input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(256, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64))\n",
    "model.add(LeakyReLU())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='linear')) \n",
    "\n",
    "\n",
    "# 1 neuron for regression task\n",
    "#1st change lr is 0.01,early stopping patience=5\n",
    "\n",
    "# Compile the model \n",
    "lower_learning_rate = 0.01\n",
    "optimizer = Adam(learning_rate=lower_learning_rate)\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mean_absolute_error'])\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint('bestmodel_weights.h5', monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    X, intensity_values,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    verbose=1,\n",
    "    validation_data=(test_data_padded, intensity_values_test),\n",
    "    callbacks=[checkpoint,early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc1a6e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load test data\n",
    "cols = ['id', 'text', 'label', 'intensity']\n",
    "\n",
    "anger_testtarget = pd.read_csv(\"C:/projects/Test/anger-ratings-0to1.dev.target.txt\", header=None, sep='\\t', names=cols, index_col=0)\n",
    "fear_testtarget = pd.read_csv(\"C:/projects/Test/fear-ratings-0to1.dev.target.txt\", header=None, sep='\\t', names=cols, index_col=0)\n",
    "sad_testtarget = pd.read_csv(\"C:/projects/Test/joy-ratings-0to1.dev.target.txt\", header=None, sep='\\t', names=cols, index_col=0)\n",
    "joy_testtarget = pd.read_csv(\"C:/projects/Test/sadness-ratings-0to1.dev.target.txt\", header=None, sep='\\t', names=cols, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e66cafe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "testtarget_df = pd.concat([anger_testtarget, fear_testtarget, sad_testtarget, joy_testtarget])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b388cff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>intensity</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10857</th>\n",
       "      <td>@ZubairSabirPTI  pls dont insult the word 'Molna'</td>\n",
       "      <td>anger</td>\n",
       "      <td>NONE</td>\n",
       "      <td>pls dont insult word molna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10858</th>\n",
       "      <td>@ArcticFantasy I would have almost took offens...</td>\n",
       "      <td>anger</td>\n",
       "      <td>NONE</td>\n",
       "      <td>would almost took offense actually snapped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10859</th>\n",
       "      <td>@IllinoisLoyalty that Rutgers game was an abom...</td>\n",
       "      <td>anger</td>\n",
       "      <td>NONE</td>\n",
       "      <td>rutgers game abomination affront god man must ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10860</th>\n",
       "      <td>@CozanGaming that's what lisa asked before she...</td>\n",
       "      <td>anger</td>\n",
       "      <td>NONE</td>\n",
       "      <td>thats lisa asked staed raging call heh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10861</th>\n",
       "      <td>Sometimes I get mad over something so minuscul...</td>\n",
       "      <td>anger</td>\n",
       "      <td>NONE</td>\n",
       "      <td>sometimes get mad something minuscule try ruin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label intensity  \\\n",
       "id                                                                          \n",
       "10857  @ZubairSabirPTI  pls dont insult the word 'Molna'  anger      NONE   \n",
       "10858  @ArcticFantasy I would have almost took offens...  anger      NONE   \n",
       "10859  @IllinoisLoyalty that Rutgers game was an abom...  anger      NONE   \n",
       "10860  @CozanGaming that's what lisa asked before she...  anger      NONE   \n",
       "10861  Sometimes I get mad over something so minuscul...  anger      NONE   \n",
       "\n",
       "                                              clean_text  \n",
       "id                                                        \n",
       "10857                         pls dont insult word molna  \n",
       "10858         would almost took offense actually snapped  \n",
       "10859  rutgers game abomination affront god man must ...  \n",
       "10860             thats lisa asked staed raging call heh  \n",
       "10861  sometimes get mad something minuscule try ruin...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import TweetTokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# Rest of your code\n",
    "text = testtarget_df['text']\n",
    "clean_text = []\n",
    "\n",
    "for text_entry in text:\n",
    "    c = TextCleaner(text_entry)\n",
    "    c.fully_clean()\n",
    "    cleaned_text = c.text\n",
    "    clean_text.append(cleaned_text)\n",
    "\n",
    "testtarget_df['clean_text'] = clean_text \n",
    "testtarget_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3abf7da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 1s 24ms/step\n",
      "[[0.44208446]\n",
      " [0.37340182]\n",
      " [0.45451653]\n",
      " [0.46709263]\n",
      " [0.5053781 ]\n",
      " [0.5053781 ]\n",
      " [0.4636131 ]\n",
      " [0.47892603]\n",
      " [0.49047592]\n",
      " [0.49047592]\n",
      " [0.4101616 ]\n",
      " [0.53668195]\n",
      " [0.4292692 ]\n",
      " [0.5309336 ]\n",
      " [0.5309336 ]\n",
      " [0.4955299 ]\n",
      " [0.4955299 ]\n",
      " [0.45588386]\n",
      " [0.5298224 ]\n",
      " [0.49648744]\n",
      " [0.49878255]\n",
      " [0.618296  ]\n",
      " [0.42737263]\n",
      " [0.42737263]\n",
      " [0.43174595]\n",
      " [0.48799714]\n",
      " [0.5025647 ]\n",
      " [0.46430764]\n",
      " [0.49062616]\n",
      " [0.40991926]\n",
      " [0.61949456]\n",
      " [0.448307  ]\n",
      " [0.5076314 ]\n",
      " [0.5043791 ]\n",
      " [0.44730324]\n",
      " [0.51958054]\n",
      " [0.51958054]\n",
      " [0.4515415 ]\n",
      " [0.4574745 ]\n",
      " [0.41212043]\n",
      " [0.5389035 ]\n",
      " [0.5000509 ]\n",
      " [0.46662056]\n",
      " [0.46662056]\n",
      " [0.43845174]\n",
      " [0.44023347]\n",
      " [0.4049279 ]\n",
      " [0.40483302]\n",
      " [0.5446663 ]\n",
      " [0.4671961 ]\n",
      " [0.4671961 ]\n",
      " [0.53951234]\n",
      " [0.439587  ]\n",
      " [0.48484844]\n",
      " [0.48484844]\n",
      " [0.4243577 ]\n",
      " [0.43931678]\n",
      " [0.33842757]\n",
      " [0.42320567]\n",
      " [0.43799347]\n",
      " [0.3906824 ]\n",
      " [0.44506106]\n",
      " [0.434919  ]\n",
      " [0.434919  ]\n",
      " [0.4388469 ]\n",
      " [0.43825185]\n",
      " [0.41303092]\n",
      " [0.5371914 ]\n",
      " [0.5371914 ]\n",
      " [0.5968971 ]\n",
      " [0.50683874]\n",
      " [0.50683874]\n",
      " [0.6504039 ]\n",
      " [0.47378436]\n",
      " [0.4362067 ]\n",
      " [0.4671961 ]\n",
      " [0.50987434]\n",
      " [0.4811908 ]\n",
      " [0.4735364 ]\n",
      " [0.50337327]\n",
      " [0.45149034]\n",
      " [0.4370846 ]\n",
      " [0.41059643]\n",
      " [0.5212388 ]\n",
      " [0.4291473 ]\n",
      " [0.44062206]\n",
      " [0.46734887]\n",
      " [0.5083389 ]\n",
      " [0.52495515]\n",
      " [0.52495515]\n",
      " [0.47036988]\n",
      " [0.43530595]\n",
      " [0.5521321 ]\n",
      " [0.5168463 ]\n",
      " [0.5168463 ]\n",
      " [0.4132232 ]\n",
      " [0.47889826]\n",
      " [0.50713104]\n",
      " [0.4973068 ]\n",
      " [0.4963677 ]\n",
      " [0.472879  ]\n",
      " [0.40880036]\n",
      " [0.40880036]\n",
      " [0.37485462]\n",
      " [0.4544236 ]\n",
      " [0.4544236 ]\n",
      " [0.47378492]\n",
      " [0.47378492]\n",
      " [0.41546366]\n",
      " [0.47042677]\n",
      " [0.4015435 ]\n",
      " [0.46468088]\n",
      " [0.5273582 ]\n",
      " [0.5273582 ]\n",
      " [0.5293187 ]\n",
      " [0.5293187 ]\n",
      " [0.3676956 ]\n",
      " [0.4901179 ]\n",
      " [0.3491943 ]\n",
      " [0.49050155]\n",
      " [0.5376573 ]\n",
      " [0.5376573 ]\n",
      " [0.45283997]\n",
      " [0.45283997]\n",
      " [0.4706822 ]\n",
      " [0.43467283]\n",
      " [0.4397096 ]\n",
      " [0.5072132 ]\n",
      " [0.42616194]\n",
      " [0.3980317 ]\n",
      " [0.4424854 ]\n",
      " [0.50716007]\n",
      " [0.46517497]\n",
      " [0.41005588]\n",
      " [0.41005588]\n",
      " [0.3881267 ]\n",
      " [0.5925267 ]\n",
      " [0.41403052]\n",
      " [0.45399243]\n",
      " [0.4671961 ]\n",
      " [0.4671961 ]\n",
      " [0.534913  ]\n",
      " [0.52770996]\n",
      " [0.44084173]\n",
      " [0.50127256]\n",
      " [0.50508183]\n",
      " [0.40390447]\n",
      " [0.57368386]\n",
      " [0.47982967]\n",
      " [0.48163956]\n",
      " [0.48163956]\n",
      " [0.43520552]\n",
      " [0.5348287 ]\n",
      " [0.5348287 ]\n",
      " [0.3310519 ]\n",
      " [0.4967385 ]\n",
      " [0.4967385 ]\n",
      " [0.5165785 ]\n",
      " [0.4279593 ]\n",
      " [0.4279593 ]\n",
      " [0.41304567]\n",
      " [0.5110355 ]\n",
      " [0.5110355 ]\n",
      " [0.50256646]\n",
      " [0.45272285]\n",
      " [0.45272285]\n",
      " [0.5239445 ]\n",
      " [0.5239445 ]\n",
      " [0.47044748]\n",
      " [0.46842998]\n",
      " [0.46842998]\n",
      " [0.50605565]\n",
      " [0.50605565]\n",
      " [0.45994017]\n",
      " [0.45994017]\n",
      " [0.469328  ]\n",
      " [0.41619545]\n",
      " [0.4671961 ]\n",
      " [0.4671961 ]\n",
      " [0.47488588]\n",
      " [0.42812607]\n",
      " [0.48812896]\n",
      " [0.58315873]\n",
      " [0.4312433 ]\n",
      " [0.4465908 ]\n",
      " [0.4088473 ]\n",
      " [0.4088473 ]\n",
      " [0.50494885]\n",
      " [0.50494885]\n",
      " [0.37234688]\n",
      " [0.37234688]\n",
      " [0.45199594]\n",
      " [0.49668252]\n",
      " [0.49668252]\n",
      " [0.4504683 ]\n",
      " [0.6140015 ]\n",
      " [0.6140015 ]\n",
      " [0.4632501 ]\n",
      " [0.4812106 ]\n",
      " [0.4812106 ]\n",
      " [0.6269652 ]\n",
      " [0.4240613 ]\n",
      " [0.4671961 ]\n",
      " [0.48072344]\n",
      " [0.42009935]\n",
      " [0.5376555 ]\n",
      " [0.46891078]\n",
      " [0.4878825 ]\n",
      " [0.4671961 ]\n",
      " [0.47530496]\n",
      " [0.45106995]\n",
      " [0.49039507]\n",
      " [0.45390508]\n",
      " [0.42994085]\n",
      " [0.3799109 ]\n",
      " [0.3222979 ]\n",
      " [0.4433254 ]\n",
      " [0.4671961 ]\n",
      " [0.4671961 ]\n",
      " [0.39820272]\n",
      " [0.5524619 ]\n",
      " [0.49239016]\n",
      " [0.4671961 ]\n",
      " [0.41947657]\n",
      " [0.46714336]\n",
      " [0.46520397]\n",
      " [0.524976  ]\n",
      " [0.524976  ]\n",
      " [0.3598371 ]\n",
      " [0.530082  ]\n",
      " [0.4671961 ]\n",
      " [0.4467868 ]\n",
      " [0.33191258]\n",
      " [0.494908  ]\n",
      " [0.49561486]\n",
      " [0.43271762]\n",
      " [0.45002246]\n",
      " [0.48442644]\n",
      " [0.46538022]\n",
      " [0.46652514]\n",
      " [0.46880126]\n",
      " [0.44728225]\n",
      " [0.43179286]\n",
      " [0.43179286]\n",
      " [0.38392207]\n",
      " [0.5137591 ]\n",
      " [0.5137591 ]\n",
      " [0.4051505 ]\n",
      " [0.44609332]\n",
      " [0.44609332]\n",
      " [0.48759565]\n",
      " [0.39491555]\n",
      " [0.44391325]\n",
      " [0.558931  ]\n",
      " [0.39345005]\n",
      " [0.55003774]\n",
      " [0.4303039 ]\n",
      " [0.50025284]\n",
      " [0.41543192]\n",
      " [0.40105194]\n",
      " [0.51121664]\n",
      " [0.52575433]\n",
      " [0.39665505]\n",
      " [0.4454714 ]\n",
      " [0.62509   ]\n",
      " [0.40087378]\n",
      " [0.40087378]\n",
      " [0.508831  ]\n",
      " [0.508831  ]\n",
      " [0.4187395 ]\n",
      " [0.38599858]\n",
      " [0.38599858]\n",
      " [0.41284925]\n",
      " [0.5083389 ]\n",
      " [0.48719776]\n",
      " [0.49046224]\n",
      " [0.35677546]\n",
      " [0.4499033 ]\n",
      " [0.41116136]\n",
      " [0.6097942 ]\n",
      " [0.41826892]\n",
      " [0.4973068 ]\n",
      " [0.5679863 ]\n",
      " [0.49505466]\n",
      " [0.5072457 ]\n",
      " [0.5038371 ]\n",
      " [0.5038371 ]\n",
      " [0.4671961 ]\n",
      " [0.42904204]\n",
      " [0.49878255]\n",
      " [0.4671961 ]\n",
      " [0.53949916]\n",
      " [0.4348859 ]\n",
      " [0.54345983]\n",
      " [0.38879713]\n",
      " [0.49637288]\n",
      " [0.43174595]\n",
      " [0.44748688]\n",
      " [0.42352563]\n",
      " [0.4657117 ]\n",
      " [0.39793497]\n",
      " [0.42210165]\n",
      " [0.4671961 ]\n",
      " [0.6282433 ]\n",
      " [0.3744645 ]\n",
      " [0.40089872]\n",
      " [0.5204351 ]\n",
      " [0.5824827 ]\n",
      " [0.56483686]\n",
      " [0.514044  ]\n",
      " [0.38044408]\n",
      " [0.4419434 ]\n",
      " [0.42823043]\n",
      " [0.42823043]\n",
      " [0.53424495]\n",
      " [0.4675045 ]\n",
      " [0.41392556]\n",
      " [0.41392556]\n",
      " [0.5343601 ]\n",
      " [0.4671961 ]\n",
      " [0.459336  ]\n",
      " [0.32865185]\n",
      " [0.3757283 ]\n",
      " [0.4554426 ]\n",
      " [0.37969446]\n",
      " [0.34811175]\n",
      " [0.40648943]\n",
      " [0.42087388]\n",
      " [0.5872795 ]\n",
      " [0.35183334]\n",
      " [0.60310197]\n",
      " [0.3843698 ]\n",
      " [0.42411593]\n",
      " [0.363209  ]\n",
      " [0.4671961 ]\n",
      " [0.57750815]\n",
      " [0.49790046]\n",
      " [0.49790046]\n",
      " [0.5372443 ]\n",
      " [0.5188066 ]\n",
      " [0.5066629 ]\n",
      " [0.4215109 ]\n",
      " [0.39818072]\n",
      " [0.48838332]\n",
      " [0.37590325]\n",
      " [0.41059643]\n",
      " [0.5143363 ]]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "# Load the model with the best weights\n",
    "best_model = load_model('model_weights_lr_changes.h5')\n",
    "\n",
    "# Tokenize the test text data using the same tokenizer used for training\n",
    "test_sequences = tokenizer.texts_to_sequences(testtarget_df['clean_text'].values)\n",
    "test_data_padded = pad_sequences(test_sequences, maxlen=X.shape[1])\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = best_model.predict(test_data_padded)\n",
    "\n",
    "# Print or use the predictions as needed\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb6a508b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              clean_text intensity  \\\n",
      "id                                                                   \n",
      "10857                         pls dont insult word molna      NONE   \n",
      "10858         would almost took offense actually snapped      NONE   \n",
      "10859  rutgers game abomination affront god man must ...      NONE   \n",
      "10860             thats lisa asked staed raging call heh      NONE   \n",
      "10861  sometimes get mad something minuscule try ruin...      NONE   \n",
      "...                                                  ...       ...   \n",
      "40855  common app randomly logged writing last pa col...      NONE   \n",
      "40856  id rather laugh rarest genius beautiful allian...      NONE   \n",
      "40857                    new stop asking invest new film      NONE   \n",
      "40858  watched django unchained people may frown titt...      NONE   \n",
      "40859  depressing despicable trump policies campaigni...      NONE   \n",
      "\n",
      "       predicted_intensity  \n",
      "id                          \n",
      "10857             0.442084  \n",
      "10858             0.373402  \n",
      "10859             0.454517  \n",
      "10860             0.467093  \n",
      "10861             0.505378  \n",
      "...                    ...  \n",
      "40855             0.398181  \n",
      "40856             0.488383  \n",
      "40857             0.375903  \n",
      "40858             0.410596  \n",
      "40859             0.514336  \n",
      "\n",
      "[347 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Assuming your test data is stored in a DataFrame called test_df\n",
    "testtarget_df['predicted_intensity'] = predictions.flatten()\n",
    "\n",
    "# Print or display the DataFrame with input test data and predicted intensity\n",
    "print(testtarget_df[['clean_text', 'intensity', 'predicted_intensity']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b713500c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 1s 25ms/step\n",
      "Pearson Correlation: 0.3033\n",
      "Spearman Correlation: 0.2876\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "# Assuming you have predictions for the test set\n",
    "predictions = model.predict(test_data_padded)\n",
    "\n",
    "# Calculate Pearson correlation\n",
    "pearson_corr, _ = pearsonr(predictions.flatten(), intensity_values_test)\n",
    "print(f'Pearson Correlation: {pearson_corr:.4f}')\n",
    "\n",
    "# Calculate Spearman correlation\n",
    "spearman_corr, _ = spearmanr(predictions.flatten(), intensity_values_test)\n",
    "print(f'Spearman Correlation: {spearman_corr:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18a9acff",
   "metadata": {},
   "outputs": [],
   "source": [
    "testtarget_df[['text','clean_text','predicted_intensity']].to_csv('LSTM_output_3666.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e4b28b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "testtarget_df[['text','clean_text','predicted_intensity']].to_csv('LSTM_output_3666.txt', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3908d63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
